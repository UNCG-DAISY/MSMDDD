{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis for Twitter for Irma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modules are used to support the analysis of tweet text using Word2Vec[^1]\n",
    "\n",
    "[^1]: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize.casual import reduce_lengthening\n",
    "from scipy.spatial.distance import cosine\n",
    "import re\n",
    "import string\n",
    "from math import sqrt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following explicitly defines the column headers as related to the CSV file containing the tweets and their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id',\n",
    "           'timestamp',\n",
    "           'tweet_text',\n",
    "           'user_id',\n",
    "           'tweet_coords',\n",
    "           'tweet_coords_list',\n",
    "           'tweet_long',\n",
    "           'tweet_lat',\n",
    "           'location',\n",
    "           'enc_url',\n",
    "           'tweet_lang',\n",
    "           'hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial analysis of the text focused on the 24 hour period centered on Hurricane Irma landfall, which occurred around 2017-09-10 13:00 GMT.\n",
    "\n",
    "The times are ```2017-09-10 00:00:00 to 2017-09-11 00:00:00```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this analysis is essentially _time-based polysemy_, i.e. the meaning of words potentially changes over time. As such, the tweets were isolated to a single language to eliminate problems that may occur from interlingual homographs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_en = tweet_full[tweet_full['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets will be scored based upon their component words' relatedness to a search, or seed, term. In this case, the intention is to determine how closely a tweet (as a whole) will be related to the word ```irma```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'irma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets were coded based upon a human-interpreted relatedness to Hurricane Irma. Tweets identified by a human to be related to Irma were given a boolean True (1) and the remainder given a boolean False (0).\n",
    "\n",
    "These codes were then matched with the initial data import, joined on the `tweet_id` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_tweets = pd.read_csv(r'./irma_related_tweets.csv')\n",
    "\n",
    "tweets_on_date = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "\n",
    "tweet_encoded = pd.concat([coded_tweets.reset_index(),tweets_on_date.iloc[:-1].reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "##     takes input string and converts or removes characters depending on settings.\n",
    "##     returns a string\n",
    "##     convert case:\n",
    "    tweet = tweet.lower()\n",
    "##    remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "##     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "##     remove non-hashtag punctuation:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "##     convert non-hashtag punctuation to whitespace:\n",
    "    tweet = tweet.translate(tweet.maketrans(string.punctuation.replace(\"#\",\"\"),\" \"*len(string.punctuation.replace(\"#\",\"\"))))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('\\B#\\B','',tweet)\n",
    "##     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "##     drop numbers.\n",
    "    tweet = re.sub(r'\\b(?<!#)\\d+\\b','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_reduce_len = reduce_lengthening(cleaned_tweet)\n",
    "#     tweet_as_tokens = word_tokenize(tweet_reduce_len)\n",
    "    tweet_as_tokens = tweet_reduce_len.split()\n",
    "    tweet_no_stops = [stemmer.stem(word) for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following four functions take a tweet and compare it to the neural network trained on a set of tweets. The result is a single scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanCosSim(tweet,vector_set):\n",
    "    tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "    score = 0\n",
    "    for i in tweet_as_terms:\n",
    "        if i in vector_set.wv.vocab:\n",
    "            score += vector_set.wv.similarity(i,search_term)\n",
    "    if len(tweet_as_terms) > 0:\n",
    "        score /= len(tweet_as_terms)\n",
    "    else:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def sumCosSimSqrtSum(tweet,vector_set):\n",
    "    tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "    score = 0\n",
    "    for i in tweet_as_terms:\n",
    "        if i in vector_set.wv.vocab:\n",
    "            score += vector_set.wv.similarity(i,search_term)\n",
    "    if len(tweet_as_terms) > 0:\n",
    "        score /= sqrt(len(tweet_as_terms))\n",
    "    else:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def dotProductOfTweetMatrixTermVector(tweet,vector_set):\n",
    "    tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "#     initialize vector with dimensionality of the vector set.\n",
    "    vector_dim = len(vector_set.wv.vectors[0])\n",
    "    score_matrix = np.zeros(vector_dim,) \n",
    "#     iterate over each word after processing. If the word is in the vocabulary,\n",
    "#     add its vector's value to the score matrix.\n",
    "#     this essentially treats a word not in the vocabulary as a zero-vector.\n",
    "    for i in tweet_as_terms:\n",
    "        if i in vector_set.wv.vocab:\n",
    "            score_matrix = np.add(score_matrix,vector_set.wv.get_vector(i))\n",
    "#     if the number of words remaining in the tweet after processing is equal to zero, return zero.\n",
    "#     otherwise, take the dot product of the score vector, and the vector of the search term.\n",
    "    if len(tweet_as_terms) > 0:\n",
    "        score = np.dot(score_matrix,vector_set.wv.get_vector(search_term))\n",
    "    else:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def cosSimOfTweetMatrixTermVector(tweet,vector_set):\n",
    "    tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "#     initialize vector with dimensionality of the vector set.\n",
    "    vector_dim = len(vector_set.wv.vectors[0])\n",
    "    score_matrix = np.zeros(vector_dim,) \n",
    "#     iterate over each word after processing. If the word is in the vocabulary,\n",
    "#     add its vector's value to the score matrix.\n",
    "#     this essentially treats a word not in the vocabulary as a zero-vector.\n",
    "    for i in tweet_as_terms:\n",
    "        if i in vector_set.wv.vocab:\n",
    "            score_matrix = np.add(score_matrix,vector_set.wv.get_vector(i))\n",
    "#     if the number of words remaining in the tweet after processing is equal to zero, return zero.\n",
    "#     otherwise, take the pairwise cosine of the score vector and the vector of the search term.\n",
    "    if ((len(tweet_as_terms) > 0) & (np.all(score_matrix != np.zeros(vector_dim,)))):\n",
    "        score = 1 - cosine(score_matrix,vector_set.wv.get_vector(search_term))\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper function that takes the tweet and the set of vectors from the neural network, and provides a way to select from the above four scalar functions as a keyword argument (default = Dot Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalarFunctions(tweet,vector_set,formula='dp'):\n",
    "    if(formula.upper() == 'DP'):\n",
    "        return dotProductOfTweetMatrixTermVector(tweet,vector_set)\n",
    "    elif(formula.upper() == 'MCS'):\n",
    "        return meanCosSim(tweet,vector_set)\n",
    "    elif(formula.upper() == 'SCSSC'):\n",
    "        return sumCosSimSqrtSum(tweet,vector_set)\n",
    "    elif(formula.upper() == 'CSTVS'):\n",
    "        return cosSimOfTweetMatrixTermVector(tweet,vector_set)\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the mechanism for training the Word2Vec neural network using the optimized parameters determined by the maximum value for the AU-ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking values from tests for max AU-ROC\n",
    "HD=150\n",
    "MWC=5\n",
    "WWS=1\n",
    "NS=1\n",
    "EP=25\n",
    "\n",
    "tweet_text = tweet_full_en.loc[tweet_date:tweet_date + date_delta,\"tweet_text\"]\n",
    "\n",
    "tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "\n",
    "opt_vector_model = Word2Vec(tweets_tokens,\n",
    "                            min_count=MWC,\n",
    "                            window=WWS,\n",
    "                            workers=1,\n",
    "                            size=HD,\n",
    "                            seed=1,\n",
    "                            sg=1,\n",
    "                            negative=NS)\n",
    "\n",
    "opt_vector_model.train(tweets_tokens, total_examples=len(tweet_text), epochs=EP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will score each tweet using each of the four scalar formulas and append those scores to a dataframe. It also min-max scales the scores based upon the values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal parameters for Word2Vec:\n",
    "for i in ['dp','mcs','scssc','cstvs']:\n",
    "#     create strings for column heads\n",
    "    sw = f'o-sw-{i}'\n",
    "    mmsw = f'MM-{sw}'\n",
    "    \n",
    "#     Scoring tweets in this column:\n",
    "    tweet_encoded[sw] = tweet_encoded.tweet_text.apply(scalarFunctions,args=(opt_vector_model,i))\n",
    "                                                       \n",
    "#     column of scores for this iteration|\n",
    "    tweet_scores = tweet_encoded[sw]\n",
    "\n",
    "#     calculating Min Max Scaling for this column \n",
    "    tweet_encoded[mmsw] = ((tweet_scores - tweet_scores.min())* 100) / (tweet_scores.max() - tweet_scores.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
